# Fact extraction

This scenario explores extraction of information from reports for integrating with some other data or process. Here, we pick reports from one day - 18 August 1989, and (for the time being) focus on one particular event - the assassination of Colonel Waldemar Franklin Quintero.

The approach is based on linked data. Various processes can operate on documents in the corpus and generate RDF triples. These processes may orchestrated for some directed purpose, or may be independent but share a vocabulary. Sets of triples may be merged in various combinations, manipulated, filtered, edited and enriched in varied and interesting ways. This sort of thing is easier to explain with a worked example...

## A worked example

Consider these sources of linked data:

1. [Metadata](https://github.com/dstl/muc3/wiki/Metadata) - as might be generated by some document processing pipeline that is applied to all reports received. For this scenario, we're interested in the [documents](http://dstl.github.io/muc3/metadata/documents.rdf), [publisher](http://dstl.github.io/muc3/metadata/publisher.rdf) and [description](http://dstl.github.io/muc3/metadata/description.rdf) RDF files that might reasonably be expected from a pipeline.

2. [Extracted Information](https://github.com/dstl/muc3/wiki/Extracted-Information) - as might be created text analysis agents, human or machine, and where multiple agents might collaborate to produce a result. The scenaro here concerns an assassination event - so the [assassinations](http://dstl.github.io/muc3/events/assassination.rdf) RDF is of primary interest, but [kidnappings](http://dstl.github.io/muc3/events/kidnap.rdf) and [bombings/explosions](http://dstl.github.io/muc3/events/explosion.rdf) can be added to the mix too. 

Loading these into the same triplestore makes it's possible to create a register for reports on a single day that give this basic information. For example this query...

```
PREFIX dc: <http://purl.org/dc/elements/1.1/>
prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>
prefix skos: <http://www.w3.org/2004/02/skos/core#>
prefix gn: <http://www.geonames.org/ontology#>
prefix schema: <http://schema.org/>
prefix foaf: <http://xmlns.com/foaf/0.1/>
prefix dbo: <http://dbpedia.org/ontology/>

select ?doc ?coverage ?title ?publisher ?name ?description ?event ?eventPage
where {

  ?doc dc:date "1989-08-18"^^xsd:date; dc:title ?title; dc:coverage ?coverage.
  OPTIONAL {
    ?snippet rdfs:member ?doc.
    ?snippet rdf:type schema:Action.
    ?snippet schema:name ?name.
  }
  OPTIONAL {
    ?doc dc:description ?description.
    ?doc dc:publisher ?publisher.
  }
  OPTIONAL {
    ?doc schema:mentions ?event.
    ?event foaf:isPrimaryTopicOf ?eventPage.
  }
}
```

...generates **[THIS RESULT](http://dstl.github.io/muc3/demo/daily.html)**. (which is [this SPARQL XML results file](input/query.srx) transformed to HTML with [this XSL stylesheet](xsl/query-html.xsl)).

Note the "pushpin" in the last column of the HTML table. This represents a link to extracted facts for some specific event mentioned (i.e. the Quintero assassination) in the report. It's picked out by one of the optional clauses in the query above:

```
 OPTIONAL {
    ?doc schema:mentions ?event.
    ?event foaf:isPrimaryTopicOf ?eventPage.
  }
```

We're slightly ahead of ourselves in that we don't have the relevant triples in our triplestore just yet. To get them, we imagine some analysis process that identifies an event of interest, finds any reports that describe it, and marks up the facts in these reports with [RDfa](https://github.com/dstl/muc3/wiki/Rdfa). Here, this has been done manually. However a human-agent collaboration is easily imagined where a machine agent does named entity recognition and disambiguation, and a human agent refines the result (giving the human the final say, for the time being at least).

Each specific event (marked-up as a schema.org [Action](https://schema.org/Action)) has the same URI in each document that mentions it. We can extract RDF triples from the RDFa marked-up reports:

* Firstly, to get a map of which event is mentioned in which report (like [this](triples/mentions_ttl.txt)). These are the triples that cause the "pushpin" to appear in the [daily register](http://dstl.github.io/muc3/demo/daily.html).

* Secondly, to capture the facts in each report as RDF triples. In this case, we have the same event described (on the day) in 3 different reports. Each of these creates a separate set of triples, but - because the event has the same URI in each report - these sets merge together when loaded into a triplestore. In this particular worked example, the extracted facts about the event are complementary (one report has a description of the vehicle involved but no precise time, and another pinpoints the time but doesn't describe the vehicle) and merge to generate a complete description of the event not found in any single report. 

Once we have an event described in the triplestore, its a simple matter to get all the triples for the event with a SPARQL DESCRIBE query...
  
 ```
DESCRIBE <http://dbpedia.org/resource/Waldemar_Franklin_Quintero#Assassination>
 ```
which gives [this RDF](input/describe1.rdf), which can be transformed to HTML (with [this XSL stylesheet](xsl/describe-html.xsl)) to give **[THIS RESULT](http://dstl.github.io/muc3/demo/event_quintero1.html)**.

A few weeks later, a fourth report ([DEV-MUC3-0446](http://dstl.github.io/muc3/dev/DEV-MUC3-0446.xhtml)) mentions arrests in connection with the murder of Quintero. Processing this report in the same way adds two new "agent" properties to the event, like **[THIS](http://dstl.github.io/muc3/demo/event_quintero2.html)**.

## Linking to other sources 

Let's try and justify this linked data approach by linking our assassination event to other facts outside this corpus. We can imagine some [FOAF](https://en.wikipedia.org/wiki/FOAF_(ontology)) linked data that includes social media accounts - [this](triples/account_ttl.txt), for example. If these FOAF triples use the same URI for a specific individual as the one used in the RDFa mark-up (or equivalence between URI's is asserted with an owl:sameAs), then merging the FOAF triples with the event triples will add the account information for that individual to the event.

You can merge the triples just by loading the two relevant files ([event](input/describe2.rdf) and [foaf](triples/account_ttl.txt)) into the triplestore of your choice, or you can do it in code - here's a snippet using [Apache Jena](https://jena.apache.org/):

```java
import java.io.FileInputStream;
import java.io.FileWriter;
import java.io.IOException;

import org.apache.jena.rdf.model.Model;
import org.apache.jena.rdf.model.ModelFactory;
import org.apache.jena.riot.Lang;
import org.apache.jena.riot.RDFDataMgr;
import org.apache.jena.util.FileManager;

public class MergeAccountInfo {

       public static void main(String[] args) throws IOException {

        Model m = ModelFactory.createDefaultModel();

        FileManager.get().readModel( m, "input/describe2.rdf" );
              RDFDataMgr.read(m, new FileInputStream("triples/account_ttl.txt"), Lang.TTL);
        
        System.out.println( String.format( "The model contains %d triples", m.size() ) );
        
        m.write(new FileWriter("triples/merged.rdf"));

       }
}
```
The [merged RDF](triples/merged.rdf) can then be rendered as HTML, exactly as above, to give **[THIS RESULT](http://dstl.github.io/muc3/demo/event_quintero3.html)**.

